<p>A computer science post-grad from Indiana University has <a href="http://blog.theincredibleholk.org/blog/2013/06/28/announcing-the-release-of-harlan/" target="_blank">publicly released</a> a new <a href="http://www.wired.com/wiredenterprise/2013/07/gpu-programming-language/" target="_blank">programming language</a> that he hopes will let developers harness the parallel processing powers of powerful modern <a href="http://" target="_blank"></a> to deliver <a href="http://www.fastcolabs.com/3013737/parallella-the-cheap-palmtop-supercomputer-goes-open-source" target="_self">supercomputer</a>-like results.</p>

<p>Harlan is distinct from other GPU-centric languages like <a href="http://www.nvidia.com/object/cuda_home_new.html" target="_blank">Nvidia's CUDA</a> or OpenCL because it's been more or less created from scratch and borrows liberally from the design of higher-level programming languages. The idea was to create a new approach to programming GPUs to access their multi-threaded potential without saddling coders with the quirks of some of rival languages or having to manage low-level hardware layers. In fact, Harlan's syntax is derived at its core from Lisp, the late 1950s language that was invented to enable artificial intelligence programming. Holk notes that as it stands Harlan has support for rich data structures including trees and ragged arrays.</p>

<p>If you're looking for a programming challenge, and like the idea of creating what may prove to be highly efficient GPU code--possibly even pushing the limits of what the hardware can manage--it's available in a <a href="https://github.com/eholk/harlan" target="_blank">GitHub repository</a>.</p>

<p>[<em>Image: By Flickr user <a href="http://www.flickr.com/gbpublic" target="_blank"> GBPublic_PR</a></em>]</p>