<p>Google may be the dominant search engine, but it’s far from ideal. One major problem: how do you search for things you don’t know exist?</p>

<p>Using Google’s own experimental algorithms, a graduate student may have build a solution: a search engine that <a href="http://insightdatascience.com/blog/thisplusthat_a_search_engine_that_lets_you_add_words_as_vectors.html" target="_blank">allows you to add and subtract search terms for far more intuitive results</a>.</p>

<p>The new search engine, <a href="http://www.thisplusthat.me/" target="_blank">ThisPlusThat.Me</a>, similarly looks for context clues among the terms. For instance: enter the arithmetic search “Paris - France + Italy” gives the top result as “Rome”, but if I search the same thing in Google, I’ll get directions between Paris and Italy, restaurants in France and Italy, and a depressing Yahoo! Answers of <a href="http://answers.yahoo.com/question/index?qid=20110425064530AA3bLPQ" target="_blank">whether Italy is in Paris (or vice versa)</a>. “Rome”, on the other hand, is an association you, a human, would make (I want <em>This</em>, without <em>That</em> but including <em>Those</em>)--and the engine makes that decision based on each answer’s semantic value compared to your search.</p>

<p>Until now, search has been stuck in a paradigm of literal matching, unable to break into conceptual associations and guessing what you <em>mean</em> when you search. There’s a reason Amazon and Netflix have scored points for their item suggestions: they’re thinking how you think.</p>

<p>The engine, created by Astrophysics Ph.D. candidate Christopher Moody, uses Google’s own open source <a href="https://code.google.com/p/word2vec/" target="_blank">word2vec</a> algorithm research to take the terms you searched for and ranks the query results by relevance, just like a normal search--except the rankings are based on “vector distances” that have a lot more human sense. So in the above example, other results could have been, say, Napoleon or wine--both have ties with the above search terms, but within the context of City - Country + Other Country, Rome is the vector that has the closest “distance”.</p>

<p>All the word2vec algorithm needs is an appropriate corpus of data to build its word relations on: Moody used Wikipedia’s corpus as a vocabulary and relational base--an obvious advantage in size, but it also had the added benefit of “canonicalizing” terms (is it <a href="http://en.wikipedia.org/wiki/Paris" target="_blank">Paris the city</a>, or <a href="http://en.wikipedia.org/wiki/Paris_%28mythology%29" target="_blank">Paris from the Trojan War</a>? In Wikipedia, the first is ‘Paris’ and the second ‘Paris_(mythology)’). But millions of search-and-replaces in Wiki’s 42 GB of text was intensive, so Moody used Hadoop’s Map functions to fan those search-and-replaces to several nodes.</p>

<p><img src="http://insightdatascience.com/img/word2vec_diagram.png" alt="" /></p>

<p>A search query then spits out an 8 GB table of vectors with varying distances; Moody tried out a few data search systems before settling on Google’s <a href="https://code.google.com/p/numexpr/" target="_blank">Numexpr</a> to find the term with the closest vector distance.</p>