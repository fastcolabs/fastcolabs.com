<p>The great promise--and great fear--of Artificial Intelligence has always been that some day, computers would be able to mimic the way our brains work. However, after years of progress, AI isn’t just a long way from <a href="https://www.youtube.com/watch?v=HwBmPiOmEGQ" target="_blank">HAL 9000</a>, it has gone in an entirely different direction. Some of the biggest tech companies in the world are beginning to implement AI in some form, and it looks nothing like we thought it would.</p>

<p>In <a href="http://www.bbc.com/future/story/20131217-weve-created-alien-intelligence/all" target="_blank">a piece for the BBC’s website</a>, writer Tom Chatfield examines the recent AI initiatives from companies like Facebook--which <a href="http://www.google.com/hostednews/afp/article/ALeqM5gQt4TTzVba-56NCC0qA3a-QXww9w?docId=d7d9f6c6-41f2-4d12-bf5d-d7599b3f9944" target="_blank">announced last week</a> that it would be partnering with NYU to build an artificial intelligence team that hopes to develop a computer that will develop insights from enormous data sets--and argued that such developments are completely contrary to the <a href="https://en.wikipedia.org/wiki/Dartmouth_Conferences" target="_blank">classic definition of AI as a field</a>.</p>

<p>Chatfield’s argument is centered on <a href="http://www.theatlantic.com/magazine/archive/2013/11/the-man-who-would-teach-machines-to-think/309529/" target="_blank">a feature in The Atlantic</a> on cognitive scientist Douglas Hofstadter, who believes that what Facebook is doing, along with other recent advances like <a href="http://www-03.ibm.com/innovation/us/watson/" target="_blank">IBM’s Watson</a>, doesn’t qualify as ’intelligence.’ Writes Chatfield:</p>

<blockquote><p>“For Hoftstadter, the label “intelligence” is simply inappropriate for describing insights drawn by brute computing power from massive data sets – because, from his perspective, the fact that results appear smart is irrelevant if the process underlying them bears no resemblance to intelligent thought. As he put it to interviewer James Somers, ‘I don’t want to be involved in passing off some fancy program’s behaviour for intelligence when I know that it has nothing to do with intelligence. And I don’t know why more people aren’t that way.’”</p></blockquote>

<p>To that end, Chatfield argues that we’ve created something entirely different. Instead of machines that think like humans, we now have machines that think in an entirely different, perhaps even alien, way. Continuing to shoehorn them into replicating our natural thought processes could be limiting.</p>

<p>Some are inclined to agree. Writing for the MIT Technology Review, Tom Simonite reiterates just how bad computers are at tasks that are easy for brains, like image recognition. Simonite attributes this to <a href="http://www.technologyreview.com/featuredstory/522476/thinking-in-silicon/" target="_blank">the way we’ve been building computer chips</a>. Namely, that it’s going to be impossible for computers to imitate non-linear thought processes if we continue to use hardware that’s designed to execute linear sequences of instructions--the CPU-RAM design called the Von Neumann architecture. Instead, an answer may lie with neuromorphic chips like IBM’s <a href="http://www.research.ibm.com/cognitive-computing/neurosynaptic-chips.shtml#fbid=B0Vdto4_Pi_" target="_blank">Synapse</a>, which are specifically designed to work the way our brains do.</p>

<p>The problem, Simonite writes, will be making them work on a larger scale. “It is still unclear whether scaling up these chips will produce machines with more sophisticated brainlike faculties. And some critics doubt it will ever be possible for engineers to copy biology closely enough to capture these abilities.”</p>

<p>As it turns out, copying biology is really damn hard. While scientists like Hofstadter prop up the platonic ideal of AI as a computer that functions the same way our brains do, perhaps the <a href="http://www.technologyreview.com/featuredstory/513696/deep-learning/" target="_blank">Deep Learning approach embraced by Google</a> is the means by which we get there. Maybe you don’t need neuromorphic chips to build a real-life HAL. Maybe you just need lots and lots of data.</p>