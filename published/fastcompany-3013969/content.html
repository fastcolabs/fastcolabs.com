<p>Twitter looks like a simple thing, doesn't it? A simple network of people who follow each other, short snippet messages of 140 characters or less, no tricky privacy controls, VIP friends, or any of the shenanigans of Facebook. When it started, it absolutely was this simple from an infrastructure point of view. But Twitter's Raffi Krikorian, VP of Engineering, just gave a presentation that points out that keeping Twitter's <a href="http://www.fastcodesign.com/1663177/design-crime-cutesy-fail-whale-error-messages-are-bad-business" target="_self">fail whale</a> from appearing for hundreds of millions of users around the world was far from the simple task of scaling up the early system. The mammoth job of simply making sure a tweet from a popular user navigates the infrastructure and gets out to the community on time may make you think twice about complaining about your own database problems.</p>

<p>Krikorian revealed some figures that show the scale of Twitter's problems: It has 150 million active users around the world, and the data going to and from these folk--that's 400 million tweets a day--squeezes through a 22 MB/second <a href="http://www.fastcompany.com/3013208/these-amazing-twitter-metadata-visualizations-will-blow-your-mind" target="_self">firehose</a>. If <a href="http://blog.fastcompany.com/post/50095502919/what-rihanna-lady-gaga-and-britney-spears-eat" target="_self">Lady Gaga</a>, with 31 million followers, sends a tweet it can take up to about 5 minutes for those short 140 characters to reach her fans. Because Twitter is much more of a consumption platform than an input platform, the company has configured its entire infrastructure to support the bias: The code actually does a lot of processing the moment tweets arrive to figure out where they need to go--this means when tweets are "read" through an <a href="http://www.fastcolabs.com/3002016/reverse-engineering-twitter-solve-advertising-mystery" target="_self">API call</a>, the process is much quicker than if the processing happened at this point.</p>

<p>There are several other tricks Twitter uses, such as keeping track of active users and storing their data more accessibly than occasional user's info, and storing a bunch of data in RAM for speedy lookups. By a bunch, I mean a lot: Every active user's code is stored in RAM to lower latencies.</p>

<p>Propelling a lot of Twitter's thinking about its infrastructure is that it's now no longer a simple web app, or even a smartphone app: It's a coherent set of APIs for delivering messages accurately on a vast scale and in near-real-time to a diverse userbase. It's this API set that effectively is Twitter's core asset, and tied to advertising it's the key to more revenue in the future.</p>

<p>It's worth reading the pr√©cis of Krikorian's talk at <a href="http://highscalability.com/blog/2013/7/8/the-architecture-twitter-uses-to-deal-with-150m-active-users.html" target="_blank">HighScalability.com</a> and the original <a href="http://www.infoq.com/presentations/Twitter-Timeline-Scalability" target="_blank">content itself</a> to learn more details. You may even glean some ideas for your next big data project.</p>

<p>[<em>Image: By Flickr user <a href="http://www.flickr.com/elsie" target="_blank"> Les Chatfield</a></em>]</p>